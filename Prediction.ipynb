{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456ddd4f-465b-4c33-8aae-c7fcc991bb26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c1543b-05b8-47a9-8288-7402a3f8cb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0568c835-afc2-42e7-ac4d-99fd8d153eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 이상치 판단\n",
    "def outlier(data, column):\n",
    "    q25 = np.quantile(data[column].dropna(), 0.25)\n",
    "    q75 = np.quantile(data[column].dropna(), 0.75)\n",
    "    iqr = q75 - q25\n",
    "    iqr_cut = iqr * 3\n",
    "    result = data[(data[column] > q75 + iqr_cut) | (data[column] < q25 - iqr_cut)].index\n",
    "    return result\n",
    "\n",
    "# 보간\n",
    "def time_interpolate(data, column):\n",
    "    tem = data[[\"일시\", column]].copy()\n",
    "    tem.index = pd.to_datetime(tem['일시'])\n",
    "    tem = tem.drop([\"일시\"], axis = 1)\n",
    "    tem = tem.interpolate(method=\"time\")\n",
    "    return tem[column].values\n",
    "\n",
    "# 강수량은 기상청에서 정한 강수표현에 따라 구간을 나누는 것으로 수치 변경\n",
    "# 비가 내리지 않음 : 0, 매우 약한 비 : 0~1, 약한 비 : 1~3, 보통 비 : 3~15, 강한 비 : 15~30, 매우 강한 비 : 30 이상\n",
    "train.강수량 = pd.cut(train.강수량, bins = [0, 0.9, 2.9, 14.9, 29.9, max(train.강수량)], labels = [1, 2, 3, 4, 5])\n",
    "train.강수량 = train.강수량.astype('float')\n",
    "train.강수량 = train.강수량.fillna(0)\n",
    "\n",
    "rain = pd.get_dummies(train.강수량)\n",
    "rain.columns = [\"비안내림\", \"매우약한비\", \"약한비\", \"보통비\", \"강한비\", \"매우강한비\"]\n",
    "train = pd.concat([train, rain.astype(\"int\")], axis=1)\n",
    "train = train.drop([\"강수량\"], axis = 1)\n",
    "\n",
    "train.최고기온 = time_interpolate(train, \"최고기온\")\n",
    "train.최저기온 = time_interpolate(train, \"최저기온\")\n",
    "\n",
    "train.일교차 = train.최고기온 - train.최저기온\n",
    "\n",
    "train.평균풍속 = time_interpolate(train, \"평균풍속\")\n",
    "\n",
    "train.일조합 = time_interpolate(train, \"일조합\")\n",
    "\n",
    "train.loc[0:4749, \"일사합\"] = 0\n",
    "train.loc[4780:4854, \"일사합\"] = 0\n",
    "train.일사합 = time_interpolate(train, \"일사합\")\n",
    "\n",
    "\n",
    "# 삭제\n",
    "train = train.iloc[train.일조율.dropna().index]\n",
    "\n",
    "# 가조합\n",
    "train[\"가조합\"] = train.일조합/(train.일조율/100)\n",
    "train.가조합 = [np.nan if i == float(\"inf\") else i for i in train.가조합]\n",
    "train.가조합 = time_interpolate(train, \"가조합\")\n",
    "\n",
    "# 일사합/일조합\n",
    "train[\"일사_일조\"] = train.일사합/train.일조합\n",
    "train.일사_일조 = [np.nan if i == float(\"inf\") else i for i in train.일사_일조] # 분모가 0인 경우 임의로 값을 설정할 수 없어 보간으로 처리\n",
    "train.일사_일조 = time_interpolate(train, \"일사_일조\")\n",
    "\n",
    "# sin + cos\n",
    "train[\"sin_cos\"] = [-np.sin(2 * np.pi * int(datetime.strptime(i,\"%Y-%m-%d\").strftime(\"%j\"))/365) - np.cos(2 * np.pi * int(datetime.strptime(i,\"%Y-%m-%d\").strftime(\"%j\"))/365) for i in train.일시]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04246ee7-9bb9-4320-874f-fb7643c546be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.일시 = train.일시.str.split(\"-\", expand = True)[0].astype(\"int\")\n",
    "train = train.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d2e1b8-c05e-42ef-bfa6-19f50a522e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year = [1980, 1990, 2000, 2005, 2010, 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65aa5e7-1f2e-4196-b2a3-842202dfff04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6939, 10592, 14244, 16071, 17897, 19723]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = []\n",
    "for i in year:\n",
    "    idx.append(train[train.일시 == i].index[0])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59587718-d311-4071-8172-261ad55ec36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop([\"일시\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2060faaa-dfe0-4852-83c0-4e42667c28e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22645 entries, 0 to 22644\n",
      "Data columns (total 18 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   최고기온     22645 non-null  float64\n",
      " 1   최저기온     22645 non-null  float64\n",
      " 2   일교차      22645 non-null  float64\n",
      " 3   평균습도     22645 non-null  float64\n",
      " 4   평균풍속     22645 non-null  float64\n",
      " 5   일조합      22645 non-null  float64\n",
      " 6   일사합      22645 non-null  float64\n",
      " 7   일조율      22645 non-null  float64\n",
      " 8   평균기온     22645 non-null  float64\n",
      " 9   비안내림     22645 non-null  int32  \n",
      " 10  매우약한비    22645 non-null  int32  \n",
      " 11  약한비      22645 non-null  int32  \n",
      " 12  보통비      22645 non-null  int32  \n",
      " 13  강한비      22645 non-null  int32  \n",
      " 14  매우강한비    22645 non-null  int32  \n",
      " 15  가조합      22645 non-null  float64\n",
      " 16  일사_일조    22645 non-null  float64\n",
      " 17  sin_cos  22645 non-null  float64\n",
      "dtypes: float64(12), int32(6)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "# 결측치 없음\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc761e74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _pywrap_tf2: 지정된 모듈을 찾을 수 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13792\\1356236784.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TF2_BEHAVIOR\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\tf2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \"\"\"\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pywrap_tf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tf2: 지정된 모듈을 찾을 수 없습니다."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0aa7b456-a3bd-4463-93c2-d138d8e0d727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, data, target, input_size, output_size, test_size):\n",
    "        self.data = data\n",
    "        self.target = target        \n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        self.col_len = len(self.data.columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3be5626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Scale(self, data):\n",
    "    scaler_in = MinMaxScaler()\n",
    "    scaler_out = MinMaxScaler()\n",
    "    \n",
    "    inputs = data.drop(self.target, axis = 1)\n",
    "    inputs_col = inputs.columns\n",
    "    outputs = data[self.target]\n",
    "    \n",
    "    scaler_in.fit(inputs)\n",
    "    inputs = pd.DataFrame(scaler_in.transform(inputs), columns = inputs_col)\n",
    "        \n",
    "    scaler_out.fit(outputs)\n",
    "    outputs = pd.DataFrame(scaler_out.fit_transform(outputs), columns = self.target)\n",
    "\n",
    "    data = pd.concat([inputs, outputs], axis = 1)\n",
    "    \n",
    "    return data,  scaler_out\n",
    "Model.Scale = Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "255df493-0982-4aa2-b0ac-641217c9b6b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Split(self, data):\n",
    "    data, _ = self.Scale(data)\n",
    "    data = tf.keras.utils.timeseries_dataset_from_array(data = data,\n",
    "                                                             targets = None,\n",
    "                                                             sequence_length = self.input_size + self.output_size)\n",
    "    inputs = np.concatenate([x[:, slice(0, self.input_size), :] for x in data], axis=0)\n",
    "    outputs = np.concatenate([x[:, slice(self.input_size, self.input_size + self.output_size), :] for x in data], axis=0)\n",
    "\n",
    "    outputs = outputs[:,:,self.col_len-1]\n",
    "    outputs = outputs.reshape(-1, self.output_size, 1)\n",
    "    \n",
    "    train_in = inputs[:int(len(inputs)*0.8), :, :]\n",
    "    train_out = outputs[:int(len(outputs)*0.8), :, :]\n",
    "    \n",
    "    test_in = inputs[int(len(inputs)*0.8):, :, :]\n",
    "    test_out = outputs[int(len(outputs)*0.8):, :, :]\n",
    "    \n",
    "    return train_in, train_out, test_in, test_out\n",
    "Model.Split = Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c274b2b7-72a7-4fca-8b4e-4862164e7931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@property\n",
    "def Data(self):\n",
    "    return self.Split(self.data)\n",
    "Model.Data = Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d24f1987-7d3c-43a8-98b2-65bb96d60638",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------\n",
      "------------------------------[ 1980 ] ---------------------------------------\n",
      "Epoch 1/100\n",
      "366/366 [==============================] - 468s 1s/step - loss: 0.0898 - mean_absolute_error: 0.0898 - val_loss: 0.0564 - val_mean_absolute_error: 0.0564\n",
      "Epoch 2/100\n",
      "366/366 [==============================] - 453s 1s/step - loss: 0.0508 - mean_absolute_error: 0.0508 - val_loss: 0.0533 - val_mean_absolute_error: 0.0533\n",
      "Epoch 3/100\n",
      "366/366 [==============================] - 465s 1s/step - loss: 0.0504 - mean_absolute_error: 0.0504 - val_loss: 0.0531 - val_mean_absolute_error: 0.0531\n",
      "Epoch 4/100\n",
      "366/366 [==============================] - 481s 1s/step - loss: 0.0500 - mean_absolute_error: 0.0500 - val_loss: 0.0517 - val_mean_absolute_error: 0.0517\n",
      "Epoch 5/100\n",
      "366/366 [==============================] - 467s 1s/step - loss: 0.0498 - mean_absolute_error: 0.0498 - val_loss: 0.0530 - val_mean_absolute_error: 0.0530\n",
      "Epoch 6/100\n",
      "366/366 [==============================] - 483s 1s/step - loss: 0.0495 - mean_absolute_error: 0.0495 - val_loss: 0.0532 - val_mean_absolute_error: 0.0532\n",
      "Epoch 7/100\n",
      "366/366 [==============================] - 476s 1s/step - loss: 0.0492 - mean_absolute_error: 0.0492 - val_loss: 0.0520 - val_mean_absolute_error: 0.0520\n",
      "Epoch 8/100\n",
      "366/366 [==============================] - 484s 1s/step - loss: 0.0489 - mean_absolute_error: 0.0489 - val_loss: 0.0525 - val_mean_absolute_error: 0.0525\n",
      "Epoch 9/100\n",
      "366/366 [==============================] - 483s 1s/step - loss: 0.0487 - mean_absolute_error: 0.0487 - val_loss: 0.0513 - val_mean_absolute_error: 0.0513\n",
      "Epoch 10/100\n",
      "366/366 [==============================] - 451s 1s/step - loss: 0.0481 - mean_absolute_error: 0.0481 - val_loss: 0.0524 - val_mean_absolute_error: 0.0524\n",
      "Epoch 11/100\n",
      "366/366 [==============================] - 436s 1s/step - loss: 0.0477 - mean_absolute_error: 0.0477 - val_loss: 0.0525 - val_mean_absolute_error: 0.0525\n",
      "Epoch 12/100\n",
      "366/366 [==============================] - 441s 1s/step - loss: 0.0471 - mean_absolute_error: 0.0471 - val_loss: 0.0524 - val_mean_absolute_error: 0.0524\n",
      "Epoch 13/100\n",
      "366/366 [==============================] - 457s 1s/step - loss: 0.0467 - mean_absolute_error: 0.0467 - val_loss: 0.0536 - val_mean_absolute_error: 0.0536\n",
      "Epoch 14/100\n",
      "366/366 [==============================] - 471s 1s/step - loss: 0.0465 - mean_absolute_error: 0.0465 - val_loss: 0.0536 - val_mean_absolute_error: 0.0536\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "----------------------------------------------------------------------------\n",
      "------------------------------[ 1990 ] ---------------------------------------\n",
      "Epoch 1/100\n",
      "275/275 [==============================] - 332s 1s/step - loss: 0.1116 - mean_absolute_error: 0.1116 - val_loss: 0.0587 - val_mean_absolute_error: 0.0587\n",
      "Epoch 2/100\n",
      "275/275 [==============================] - 341s 1s/step - loss: 0.0525 - mean_absolute_error: 0.0525 - val_loss: 0.0553 - val_mean_absolute_error: 0.0553\n",
      "Epoch 3/100\n",
      "275/275 [==============================] - 351s 1s/step - loss: 0.0519 - mean_absolute_error: 0.0519 - val_loss: 0.0539 - val_mean_absolute_error: 0.0539\n",
      "Epoch 4/100\n",
      "275/275 [==============================] - 359s 1s/step - loss: 0.0515 - mean_absolute_error: 0.0515 - val_loss: 0.0539 - val_mean_absolute_error: 0.0539\n",
      "Epoch 5/100\n",
      "275/275 [==============================] - 342s 1s/step - loss: 0.0512 - mean_absolute_error: 0.0512 - val_loss: 0.0544 - val_mean_absolute_error: 0.0544\n",
      "Epoch 6/100\n",
      "275/275 [==============================] - 339s 1s/step - loss: 0.0512 - mean_absolute_error: 0.0512 - val_loss: 0.0539 - val_mean_absolute_error: 0.0539\n",
      "Epoch 7/100\n",
      "275/275 [==============================] - 345s 1s/step - loss: 0.0509 - mean_absolute_error: 0.0509 - val_loss: 0.0540 - val_mean_absolute_error: 0.0540\n",
      "Epoch 8/100\n",
      "275/275 [==============================] - 363s 1s/step - loss: 0.0507 - mean_absolute_error: 0.0507 - val_loss: 0.0529 - val_mean_absolute_error: 0.0529\n",
      "Epoch 9/100\n",
      "275/275 [==============================] - 363s 1s/step - loss: 0.0505 - mean_absolute_error: 0.0505 - val_loss: 0.0529 - val_mean_absolute_error: 0.0529\n",
      "Epoch 10/100\n",
      "275/275 [==============================] - 363s 1s/step - loss: 0.0504 - mean_absolute_error: 0.0504 - val_loss: 0.0531 - val_mean_absolute_error: 0.0531\n",
      "Epoch 11/100\n",
      "275/275 [==============================] - 362s 1s/step - loss: 0.0501 - mean_absolute_error: 0.0501 - val_loss: 0.0534 - val_mean_absolute_error: 0.0534\n",
      "Epoch 12/100\n",
      "275/275 [==============================] - 325s 1s/step - loss: 0.0503 - mean_absolute_error: 0.0503 - val_loss: 0.0544 - val_mean_absolute_error: 0.0544\n",
      "Epoch 13/100\n",
      "275/275 [==============================] - 326s 1s/step - loss: 0.0499 - mean_absolute_error: 0.0499 - val_loss: 0.0529 - val_mean_absolute_error: 0.0529\n",
      "Epoch 14/100\n",
      "275/275 [==============================] - 327s 1s/step - loss: 0.0497 - mean_absolute_error: 0.0497 - val_loss: 0.0542 - val_mean_absolute_error: 0.0542\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "----------------------------------------------------------------------------\n",
      "------------------------------[ 2000 ] ---------------------------------------\n",
      "Epoch 1/100\n",
      "184/184 [==============================] - 229s 1s/step - loss: 0.1385 - mean_absolute_error: 0.1385 - val_loss: 0.0578 - val_mean_absolute_error: 0.0578\n",
      "Epoch 2/100\n",
      "184/184 [==============================] - 238s 1s/step - loss: 0.0545 - mean_absolute_error: 0.0545 - val_loss: 0.0556 - val_mean_absolute_error: 0.0556\n",
      "Epoch 3/100\n",
      "184/184 [==============================] - 236s 1s/step - loss: 0.0526 - mean_absolute_error: 0.0526 - val_loss: 0.0544 - val_mean_absolute_error: 0.0544\n",
      "Epoch 4/100\n",
      "184/184 [==============================] - 238s 1s/step - loss: 0.0526 - mean_absolute_error: 0.0526 - val_loss: 0.0558 - val_mean_absolute_error: 0.0558\n",
      "Epoch 5/100\n",
      "184/184 [==============================] - 250s 1s/step - loss: 0.0521 - mean_absolute_error: 0.0521 - val_loss: 0.0555 - val_mean_absolute_error: 0.0555\n",
      "Epoch 6/100\n",
      "184/184 [==============================] - 250s 1s/step - loss: 0.0515 - mean_absolute_error: 0.0515 - val_loss: 0.0548 - val_mean_absolute_error: 0.0548\n",
      "Epoch 7/100\n",
      "184/184 [==============================] - 235s 1s/step - loss: 0.0513 - mean_absolute_error: 0.0513 - val_loss: 0.0543 - val_mean_absolute_error: 0.0543\n",
      "Epoch 8/100\n",
      "184/184 [==============================] - 225s 1s/step - loss: 0.0513 - mean_absolute_error: 0.0513 - val_loss: 0.0543 - val_mean_absolute_error: 0.0543\n",
      "Epoch 9/100\n",
      "184/184 [==============================] - 225s 1s/step - loss: 0.0507 - mean_absolute_error: 0.0507 - val_loss: 0.0538 - val_mean_absolute_error: 0.0538\n",
      "Epoch 10/100\n",
      "184/184 [==============================] - 224s 1s/step - loss: 0.0507 - mean_absolute_error: 0.0507 - val_loss: 0.0540 - val_mean_absolute_error: 0.0540\n",
      "Epoch 11/100\n",
      "184/184 [==============================] - 224s 1s/step - loss: 0.0505 - mean_absolute_error: 0.0505 - val_loss: 0.0549 - val_mean_absolute_error: 0.0549\n",
      "Epoch 12/100\n",
      "184/184 [==============================] - 226s 1s/step - loss: 0.0499 - mean_absolute_error: 0.0499 - val_loss: 0.0542 - val_mean_absolute_error: 0.0542\n",
      "Epoch 13/100\n",
      "184/184 [==============================] - 225s 1s/step - loss: 0.0496 - mean_absolute_error: 0.0496 - val_loss: 0.0562 - val_mean_absolute_error: 0.0562\n",
      "Epoch 14/100\n",
      "184/184 [==============================] - 223s 1s/step - loss: 0.0491 - mean_absolute_error: 0.0491 - val_loss: 0.0562 - val_mean_absolute_error: 0.0562\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "----------------------------------------------------------------------------\n",
      "------------------------------[ 2005 ] ---------------------------------------\n",
      "Epoch 1/100\n",
      "138/138 [==============================] - 184s 1s/step - loss: 0.1641 - mean_absolute_error: 0.1641 - val_loss: 0.0656 - val_mean_absolute_error: 0.0656\n",
      "Epoch 2/100\n",
      "138/138 [==============================] - 174s 1s/step - loss: 0.0562 - mean_absolute_error: 0.0562 - val_loss: 0.0575 - val_mean_absolute_error: 0.0575\n",
      "Epoch 3/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0533 - mean_absolute_error: 0.0533 - val_loss: 0.0554 - val_mean_absolute_error: 0.0554\n",
      "Epoch 4/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0526 - mean_absolute_error: 0.0526 - val_loss: 0.0562 - val_mean_absolute_error: 0.0562\n",
      "Epoch 5/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0527 - mean_absolute_error: 0.0527 - val_loss: 0.0588 - val_mean_absolute_error: 0.0588\n",
      "Epoch 6/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0525 - mean_absolute_error: 0.0525 - val_loss: 0.0553 - val_mean_absolute_error: 0.0553\n",
      "Epoch 7/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0520 - mean_absolute_error: 0.0520 - val_loss: 0.0562 - val_mean_absolute_error: 0.0562\n",
      "Epoch 8/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0520 - mean_absolute_error: 0.0520 - val_loss: 0.0556 - val_mean_absolute_error: 0.0556\n",
      "Epoch 9/100\n",
      "138/138 [==============================] - 174s 1s/step - loss: 0.0521 - mean_absolute_error: 0.0521 - val_loss: 0.0552 - val_mean_absolute_error: 0.0552\n",
      "Epoch 10/100\n",
      "138/138 [==============================] - 174s 1s/step - loss: 0.0519 - mean_absolute_error: 0.0519 - val_loss: 0.0550 - val_mean_absolute_error: 0.0550\n",
      "Epoch 11/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0516 - mean_absolute_error: 0.0516 - val_loss: 0.0551 - val_mean_absolute_error: 0.0551\n",
      "Epoch 12/100\n",
      "138/138 [==============================] - 184s 1s/step - loss: 0.0512 - mean_absolute_error: 0.0512 - val_loss: 0.0566 - val_mean_absolute_error: 0.0566\n",
      "Epoch 13/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0512 - mean_absolute_error: 0.0512 - val_loss: 0.0557 - val_mean_absolute_error: 0.0557\n",
      "Epoch 14/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0508 - mean_absolute_error: 0.0508 - val_loss: 0.0553 - val_mean_absolute_error: 0.0553\n",
      "Epoch 15/100\n",
      "138/138 [==============================] - 175s 1s/step - loss: 0.0508 - mean_absolute_error: 0.0508 - val_loss: 0.0556 - val_mean_absolute_error: 0.0556\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002326D6DA0C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "----------------------------------------------------------------------------\n",
      "------------------------------[ 2010 ] ---------------------------------------\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 132s 1s/step - loss: 0.2113 - mean_absolute_error: 0.2113 - val_loss: 0.1360 - val_mean_absolute_error: 0.1360\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 126s 1s/step - loss: 0.0947 - mean_absolute_error: 0.0947 - val_loss: 0.0713 - val_mean_absolute_error: 0.0713\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 126s 1s/step - loss: 0.0567 - mean_absolute_error: 0.0567 - val_loss: 0.0609 - val_mean_absolute_error: 0.0609\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 126s 1s/step - loss: 0.0528 - mean_absolute_error: 0.0528 - val_loss: 0.0581 - val_mean_absolute_error: 0.0581\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 125s 1s/step - loss: 0.0515 - mean_absolute_error: 0.0515 - val_loss: 0.0582 - val_mean_absolute_error: 0.0582\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 125s 1s/step - loss: 0.0513 - mean_absolute_error: 0.0513 - val_loss: 0.0575 - val_mean_absolute_error: 0.0575\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 124s 1s/step - loss: 0.0509 - mean_absolute_error: 0.0509 - val_loss: 0.0571 - val_mean_absolute_error: 0.0571\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 125s 1s/step - loss: 0.0510 - mean_absolute_error: 0.0510 - val_loss: 0.0568 - val_mean_absolute_error: 0.0568\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 127s 1s/step - loss: 0.0507 - mean_absolute_error: 0.0507 - val_loss: 0.0573 - val_mean_absolute_error: 0.0573\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 115s 1s/step - loss: 0.0506 - mean_absolute_error: 0.0506 - val_loss: 0.0564 - val_mean_absolute_error: 0.0564\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 112s 1s/step - loss: 0.0503 - mean_absolute_error: 0.0503 - val_loss: 0.0570 - val_mean_absolute_error: 0.0570\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 112s 1s/step - loss: 0.0502 - mean_absolute_error: 0.0502 - val_loss: 0.0579 - val_mean_absolute_error: 0.0579\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 113s 1s/step - loss: 0.0502 - mean_absolute_error: 0.0502 - val_loss: 0.0570 - val_mean_absolute_error: 0.0570\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 113s 1s/step - loss: 0.0500 - mean_absolute_error: 0.0500 - val_loss: 0.0572 - val_mean_absolute_error: 0.0572\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 125s 1s/step - loss: 0.0498 - mean_absolute_error: 0.0498 - val_loss: 0.0571 - val_mean_absolute_error: 0.0571\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000232540E6FC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "----------------------------------------------------------------------------\n",
      "------------------------------[ 2015 ] ---------------------------------------\n",
      "Epoch 1/100\n",
      "47/47 [==============================] - 64s 1s/step - loss: 0.2447 - mean_absolute_error: 0.2447 - val_loss: 0.1907 - val_mean_absolute_error: 0.1907\n",
      "Epoch 2/100\n",
      "47/47 [==============================] - 60s 1s/step - loss: 0.1569 - mean_absolute_error: 0.1569 - val_loss: 0.1201 - val_mean_absolute_error: 0.1201\n",
      "Epoch 3/100\n",
      "47/47 [==============================] - 60s 1s/step - loss: 0.0784 - mean_absolute_error: 0.0784 - val_loss: 0.0689 - val_mean_absolute_error: 0.0689\n",
      "Epoch 4/100\n",
      "47/47 [==============================] - 60s 1s/step - loss: 0.0585 - mean_absolute_error: 0.0585 - val_loss: 0.0598 - val_mean_absolute_error: 0.0598\n",
      "Epoch 5/100\n",
      "47/47 [==============================] - 60s 1s/step - loss: 0.0572 - mean_absolute_error: 0.0572 - val_loss: 0.0565 - val_mean_absolute_error: 0.0565\n",
      "Epoch 6/100\n",
      "47/47 [==============================] - 60s 1s/step - loss: 0.0549 - mean_absolute_error: 0.0549 - val_loss: 0.0569 - val_mean_absolute_error: 0.0569\n",
      "Epoch 7/100\n",
      "47/47 [==============================] - 60s 1s/step - loss: 0.0542 - mean_absolute_error: 0.0542 - val_loss: 0.0572 - val_mean_absolute_error: 0.0572\n",
      "Epoch 8/100\n",
      "47/47 [==============================] - 60s 1s/step - loss: 0.0541 - mean_absolute_error: 0.0541 - val_loss: 0.0573 - val_mean_absolute_error: 0.0573\n",
      "Epoch 9/100\n",
      "47/47 [==============================] - 60s 1s/step - loss: 0.0542 - mean_absolute_error: 0.0542 - val_loss: 0.0558 - val_mean_absolute_error: 0.0558\n",
      "Epoch 10/100\n",
      "47/47 [==============================] - 66s 1s/step - loss: 0.0534 - mean_absolute_error: 0.0534 - val_loss: 0.0542 - val_mean_absolute_error: 0.0542\n",
      "Epoch 11/100\n",
      "47/47 [==============================] - 67s 1s/step - loss: 0.0535 - mean_absolute_error: 0.0535 - val_loss: 0.0591 - val_mean_absolute_error: 0.0591\n",
      "Epoch 12/100\n",
      "47/47 [==============================] - 67s 1s/step - loss: 0.0537 - mean_absolute_error: 0.0537 - val_loss: 0.0559 - val_mean_absolute_error: 0.0559\n",
      "Epoch 13/100\n",
      "47/47 [==============================] - 67s 1s/step - loss: 0.0542 - mean_absolute_error: 0.0542 - val_loss: 0.0574 - val_mean_absolute_error: 0.0574\n",
      "Epoch 14/100\n",
      "47/47 [==============================] - 67s 1s/step - loss: 0.0536 - mean_absolute_error: 0.0536 - val_loss: 0.0552 - val_mean_absolute_error: 0.0552\n",
      "Epoch 15/100\n",
      "47/47 [==============================] - 64s 1s/step - loss: 0.0535 - mean_absolute_error: 0.0535 - val_loss: 0.0549 - val_mean_absolute_error: 0.0549\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# train 길이 결정\n",
    "Year_loss = pd.DataFrame([], columns = [\"Year\", \"MAE\"])\n",
    "for i in range(len(idx)):\n",
    "    print(\"----------------------------------------------------------------------------\")\n",
    "    print(\"------------------------------[ {} ] ---------------------------------------\".format(year[i]))\n",
    "    train_c = train.copy()\n",
    "    train_c = train_c.iloc[idx[i]:]\n",
    "    \n",
    "    output_size = 358\n",
    "    model = Model(data = train_c,\n",
    "         target = [\"평균기온\"],\n",
    "         input_size = output_size * 2,\n",
    "         output_size = output_size,\n",
    "         test_size = 0.3)\n",
    "    train_in, train_out, test_in, test_out = model.Data\n",
    "    \n",
    "    lstm_model, history = LSTM_fit(model.Data)\n",
    "    \n",
    "    _, _, test_in, _ = model.Split(train_c)\n",
    "    _, scaler_out = model.Scale(train_c)\n",
    "    pred = lstm_model.predict(test_in[-1].reshape(1, (output_size*2), len(train_c.columns)))\n",
    "    pred = np.round(scaler_out.inverse_transform(pred))\n",
    "    \n",
    "    \n",
    "    mae = mean_absolute_error(train_c.iloc[-output_size:].평균기온.values, pred.reshape(-1))\n",
    "    \n",
    "    Year_loss.loc[len(Year_loss)] = [year[i],mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf8ed878-cd33-4571-ba78-b83f6c72d4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980.0</td>\n",
       "      <td>2.975140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1990.0</td>\n",
       "      <td>2.913687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2.740503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005.0</td>\n",
       "      <td>2.615363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>2.631564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>2.715363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year       MAE\n",
       "0  1980.0  2.975140\n",
       "1  1990.0  2.913687\n",
       "2  2000.0  2.740503\n",
       "3  2005.0  2.615363\n",
       "4  2010.0  2.631564\n",
       "5  2015.0  2.715363"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2005년으로 결정\n",
    "Year_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09f0cbce-a6df-4682-b8fb-a370abfae4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.iloc[idx[3]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4339ac33-8d78-43df-b938-5159c2eadf51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_size = 358\n",
    "space = {\n",
    "    'input_size' : hp.choice(\"input_size\", [output_size, output_size*2, output_size*3]),\n",
    "    'lstm1_nodes' : hp.choice(\"lstm1_nodes\", [64, 128, 256]),\n",
    "    #'lstm1_dropout' : hp.choice(\"lstm1_dropout\", [0, 0.3, 0.5]),\n",
    "    'lstm2_nodes' : hp.choice('lstm2_nodes', [64, 128, 256]),\n",
    "    #'lstm2_dropout' : hp.choice(\"lstm2_dropout\", [0, 0.3, 0.5]),\n",
    "    'num_layers' : hp.choice('num_layers',[\n",
    "       # { \n",
    "       #     'layers' : 'two',\n",
    "       # },\n",
    "        {\n",
    "            'layers' : 'three',\n",
    "            'lstm3_nodes' : hp.choice('lstm3_nodes', [32, 64, 128]),\n",
    "     #       'lstm3_dropout' : hp.choice(\"lstm3_dropout\", [0, 0.3, 0.5])\n",
    "        }\n",
    "    ]),\n",
    "    'lr' : hp.choice('lr', [0, 0.001, 0.002, 0.003])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a46e225-38ea-4424-87ce-a57f22dda435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Parameter_loss = pd.DataFrame([], columns = [\"Parameters\", \"Loss\"])\n",
    "def hyperopt_model(params):\n",
    "    global Parameter_loss\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"---------------------------------------[ START {}]-------------------------------------------------------------\".format(len(Parameter_loss)))\n",
    "    print(\"Parameter : {}\".format(params))\n",
    "    input_size = params['input_size']\n",
    "    \n",
    "    output_size = 358\n",
    "    model = Model(data = train,\n",
    "         target = [\"평균기온\"],\n",
    "         input_size = params['input_size'],\n",
    "         output_size = output_size,\n",
    "         test_size = 0.3)\n",
    "    \n",
    "    train_in, train_out, test_in, test_out = model.Data\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                      mode = 'min',\n",
    "                                                      patience = 3,\n",
    "                                                      min_delta = 0.001)\n",
    "    tf.random.set_seed = 1234\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=1234)\n",
    "    \n",
    "    lstm_model = tf.keras.Sequential()\n",
    "    lstm_model.add(tf.keras.layers.GRU(params[\"lstm1_nodes\"], \n",
    "                                       # dropout = params[\"lstm1_dropout\"],\n",
    "                                        return_sequences = True, \n",
    "                                        kernel_initializer=initializer))\n",
    "    if params[\"num_layers\"][\"layers\"] == \"two\":\n",
    "        lstm_model.add(tf.keras.layers.GRU(params[\"lstm2_nodes\"], \n",
    "                                          #  dropout = params[\"lstm2_dropout\"],\n",
    "                                            return_sequences = False, \n",
    "                                        kernel_initializer=initializer))\n",
    "    else:\n",
    "        lstm_model.add(tf.keras.layers.GRU(params[\"lstm2_nodes\"], \n",
    "                                        #    dropout = params[\"lstm2_dropout\"],\n",
    "                                            return_sequences = True, \n",
    "                                        kernel_initializer=initializer))\n",
    "        lstm_model.add(tf.keras.layers.GRU(params[\"num_layers\"][\"lstm3_nodes\"], \n",
    "                                         #   dropout = params[\"num_layers\"][\"lstm3_dropout\"],\n",
    "                                            return_sequences = False, \n",
    "                                        kernel_initializer=initializer))\n",
    "    lstm_model.add(tf.keras.layers.Dense(output_size, \n",
    "                                        kernel_initializer=initializer))\n",
    "   \n",
    "    lstm_model.compile(loss = tf.keras.losses.MeanAbsoluteError(),\n",
    "                      optimizer = tf.keras.optimizers.Adam(learning_rate = params[\"lr\"]),\n",
    "                      metrics = [tf.keras.metrics.mean_absolute_error])\n",
    "    \n",
    "    history = lstm_model.fit(train_in, train_out,\n",
    "                             epochs = 100,\n",
    "                             validation_data = [test_in, test_out],\n",
    "                             callbacks = [early_stopping],\n",
    "                            verbose = 2)\n",
    "    val_error = np.amin(history.history[\"val_loss\"])\n",
    "    Parameter_loss.loc[len(Parameter_loss)] = [params,val_error]\n",
    "    \n",
    "    print(\"val_error : {}\".format(val_error))\n",
    "    print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    return {\"loss\" : val_error, \"model\":lstm_model, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6c928-d4fb-4fd2-ab9d-99676038bb4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "trials = Trials()\n",
    "best = fmin(hyperopt_model,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 5,\n",
    "            trials = trials)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "447de0e7-bc09-4673-9eb3-ce805d6cd5d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_size': 0,\n",
       " 'lr': 2,\n",
       " 'lstm1_nodes': 2,\n",
       " 'lstm2_nodes': 1,\n",
       " 'lstm3_nodes': 0,\n",
       " 'num_layers': 0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4dfe7b78-50cf-44d6-b420-93569e5c26ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.906722295284272"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input size = 385, lr = 0.002, lstm1_nodes = 256, lstm2_nodes = 128, lstm3_nodes = 32 => 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6747515-d7a8-4d22-9dd6-f9bf66716fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_size = 358\n",
    "model = Model(data = train,\n",
    "     target = [\"평균기온\"],\n",
    "     input_size = output_size * 2,\n",
    "     output_size = output_size,\n",
    "     test_size = 0.3)\n",
    "train_in, train_out, test_in, test_out = model.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e198ed7b-1459-4d04-adb1-afd34f56aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_fit(data):\n",
    "    train_in, train_out, test_in, test_out = data\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                      mode = 'min',\n",
    "                                                      patience = 3,\n",
    "                                                      min_delta = 0.001)\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=1234)\n",
    "    lstm_model = tf.keras.Sequential()\n",
    "    lstm_model.add(tf.keras.layers.LSTM(256, return_sequences = True, \n",
    "                                        kernel_initializer=initializer))\n",
    "    lstm_model.add(tf.keras.layers.LSTM(128, return_sequences = True, \n",
    "                                        kernel_initializer=initializer))\n",
    "    lstm_model.add(tf.keras.layers.LSTM(32, return_sequences = False, \n",
    "                                        kernel_initializer=initializer))\n",
    "    lstm_model.add(tf.keras.layers.Dense(output_size, \n",
    "                                        kernel_initializer=initializer))\n",
    "   \n",
    "    lstm_model.compile(loss = tf.keras.losses.MeanAbsoluteError(),\n",
    "                      optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                      metrics = [tf.keras.metrics.mean_absolute_error])\n",
    "    \n",
    "    history = lstm_model.fit(train_in, train_out,\n",
    "                             epochs = 100,\n",
    "                             validation_data = [test_in, test_out],\n",
    "                             callbacks = [early_stopping])\n",
    "    \n",
    "    return lstm_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b9f9aa-92bc-4df8-9dca-dd19723cdb15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "329/540 [=================>............] - ETA: 18:01 - loss: 0.1853 - mean_absolute_error: 0.1853"
     ]
    }
   ],
   "source": [
    "lstm_model, history = LSTM_fit(model.Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ef3a9-cf14-4054-9721-080ca4e6b019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, _, test_in, _ = model.Split(train)\n",
    "_, scaler_out = model.Scale(train)\n",
    "pred = lstm_model.predict(test_in[-1].reshape(1, (output_size*2), len(train.columns)))\n",
    "pred = np.round(scaler_out.inverse_transform(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a03707-ab2a-4934-a75b-0a2d8fe9d3d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(train.iloc[-output_size:].평균기온.values, pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1a16c-dfe4-4519-b48b-09aedacd31e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6251bab-b0e1-46d9-a83c-2a0e1107a647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_size = 365\n",
    "model = Model(data = train,\n",
    "     target = [\"평균기온\"],\n",
    "     input_size = output_size * 2,\n",
    "     output_size = output_size,\n",
    "     test_size = 0.3)\n",
    "train_in, train_out, test_in, test_out = model.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0fa8300-ac53-45e0-8d7c-fe8840721316",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "539/539 [==============================] - 647s 1s/step - loss: 0.0826 - mean_absolute_error: 0.0826 - val_loss: 0.0554 - val_mean_absolute_error: 0.0554\n",
      "Epoch 2/100\n",
      "236/539 [============>.................] - ETA: 5:45 - loss: 0.0519 - mean_absolute_error: 0.0519"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lstm_model, history \u001b[38;5;241m=\u001b[39m LSTM_fit(model\u001b[38;5;241m.\u001b[39mData)\n",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m, in \u001b[0;36mLSTM_fit\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     12\u001b[0m lstm_model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(output_size))\n\u001b[0;32m     14\u001b[0m lstm_model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanAbsoluteError(),\n\u001b[0;32m     15\u001b[0m                   optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[0;32m     16\u001b[0m                   metrics \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mmean_absolute_error])\n\u001b[1;32m---> 18\u001b[0m history \u001b[38;5;241m=\u001b[39m lstm_model\u001b[38;5;241m.\u001b[39mfit(train_in, train_out,\n\u001b[0;32m     19\u001b[0m                          epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     20\u001b[0m                          validation_data \u001b[38;5;241m=\u001b[39m [test_in, test_out],\n\u001b[0;32m     21\u001b[0m                          callbacks \u001b[38;5;241m=\u001b[39m [early_stopping])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lstm_model, history\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1805\u001b[0m ):\n\u001b[0;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    869\u001b[0m       args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_config\n\u001b[0;32m    870\u001b[0m   )\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model, history = LSTM_fit(model.Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcde602-c9a0-4de3-99a6-8f178cec592a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, _, test_in, _ = model.Split(train)\n",
    "_, scaler_out = model.Scale(train)\n",
    "pred = lstm_model.predict(test_in[-1].reshape(1, (output_size*2), len(train.columns)))\n",
    "pred = np.round(scaler_out.inverse_transform(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede59e2f-71b8-4fa5-9562-edc7b408d8b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe03b3-d4dd-4e22-a2a6-e05161c4128c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(train.iloc[-output_size:].평균기온.values, pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68afd6c-58fc-4d6a-b1ce-67eaffb94519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8fc9d-76c2-447b-a288-18490252748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "# 1. 128, 128, 128 unit, out_put size = 358, input_size = 358 * 2  => 2.69\n",
    "# 2. 1과 동일, 계절 변수 원핫 인코딩 => 3.10\n",
    "# 3. 2와 동일, 강수량 변수 원핫 인코딩 => 2.69\n",
    "# 4. 3과 동일하지만 계절 변수 삭제 => 2.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "adbe0e4a-ab90-48e3-bcf1-4060f8c7faf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_size = 358\n",
    "model = Model(data = train,\n",
    "     target = [\"평균기온\"],\n",
    "     input_size = output_size * 2,\n",
    "     output_size = output_size,\n",
    "     test_size = 0.3)\n",
    "train_in, train_out, test_in, test_out = model.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5782b43a-1e3d-4d99-b1e6-ca9471501d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU_fit(data):\n",
    "    train_in, train_out, test_in, test_out = data\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                                      mode = 'min',\n",
    "                                                      patience = 3,\n",
    "                                                      min_delta = 0.001)\n",
    "   \n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=1234)\n",
    "    lstm_model = tf.keras.Sequential()\n",
    "    lstm_model.add(tf.keras.layers.GRU(128, return_sequences = True, \n",
    "                                        kernel_initializer=initializer))\n",
    "    lstm_model.add(tf.keras.layers.GRU(128, return_sequences = True, \n",
    "                                        kernel_initializer=initializer))\n",
    "    lstm_model.add(tf.keras.layers.GRU(128, return_sequences = False, \n",
    "                                        kernel_initializer=initializer))\n",
    "    lstm_model.add(tf.keras.layers.Dense(output_size))\n",
    "   \n",
    "    lstm_model.compile(loss = tf.keras.losses.MeanAbsoluteError(),\n",
    "                      optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                      metrics = [tf.keras.metrics.mean_absolute_error])\n",
    "    \n",
    "    history = lstm_model.fit(train_in, train_out,\n",
    "                             epochs = 100,\n",
    "                             validation_data = [test_in, test_out],\n",
    "                             callbacks = [early_stopping])\n",
    "    \n",
    "    return lstm_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ab4786c-418a-48e7-8855-dcc664fe7afa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "540/540 [==============================] - 714s 1s/step - loss: 0.0711 - mean_absolute_error: 0.0711 - val_loss: 0.0550 - val_mean_absolute_error: 0.0550\n",
      "Epoch 2/100\n",
      "540/540 [==============================] - 705s 1s/step - loss: 0.0514 - mean_absolute_error: 0.0514 - val_loss: 0.0543 - val_mean_absolute_error: 0.0543\n",
      "Epoch 3/100\n",
      "540/540 [==============================] - 717s 1s/step - loss: 0.0509 - mean_absolute_error: 0.0509 - val_loss: 0.0530 - val_mean_absolute_error: 0.0530\n",
      "Epoch 4/100\n",
      "540/540 [==============================] - 703s 1s/step - loss: 0.0506 - mean_absolute_error: 0.0506 - val_loss: 0.0533 - val_mean_absolute_error: 0.0533\n",
      "Epoch 5/100\n",
      "540/540 [==============================] - 710s 1s/step - loss: 0.0502 - mean_absolute_error: 0.0502 - val_loss: 0.0543 - val_mean_absolute_error: 0.0543\n",
      "Epoch 6/100\n",
      "540/540 [==============================] - 707s 1s/step - loss: 0.0496 - mean_absolute_error: 0.0496 - val_loss: 0.0524 - val_mean_absolute_error: 0.0524\n"
     ]
    }
   ],
   "source": [
    "GRU_model, history = GRU_fit(model.Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cc4dd830-225b-4548-8fa4-694480b510c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.8438547486033516"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, test_in, _ = model.Split(train)\n",
    "_, scaler_out = model.Scale(train)\n",
    "pred = GRU_model.predict(test_in[-1].reshape(1, (output_size*2), len(train.columns)))\n",
    "pred = np.round(scaler_out.inverse_transform(pred))\n",
    "mean_absolute_error(train.iloc[-output_size:].평균기온.values, pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "03f8df70-efab-4511-8cc5-b9e15419b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"data/sample_submission.csv\")\n",
    "sub.평균기온 = pred[0]\n",
    "sub.to_csv(\"data/2_84.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f26fe-36c5-4ac6-ab60-516d7be36d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_size = 365\n",
    "model = Model(data = train,\n",
    "     target = [\"평균기온\"],\n",
    "     input_size = output_size * 2,\n",
    "     output_size = output_size,\n",
    "     test_size = 0.3)\n",
    "train_in, train_out, test_in, test_out = model.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09311c-1bdd-403b-9d16-87a2e7c27400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GRU_model, history = GRU_fit(model.Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f5b07-d7e5-49fc-923a-6c0e153460f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, _, test_in, _ = model.Split(train)\n",
    "_, scaler_out = model.Scale(train)\n",
    "pred = GRU_model.predict(test_in[-1].reshape(1, (output_size*2), 13))\n",
    "pred = np.round(scaler_out.inverse_transform(pred))\n",
    "mean_absolute_error(train.iloc[-output_size:].평균기온.values, pred.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba513c-23dc-41cd-8308-df3ed4d64490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0af51-be23-4caa-8a44-d7420b03aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "\n",
    "# 1. 128, 128, 128 unit, out_put size = 358, input_size = 358 * 2  => 2.78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4414b-b9c3-46df-8865-6d7b63ba871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"data/sample_submission.csv\")\n",
    "scale_data, scaler_in, scaler_out = model.Scale(train)\n",
    "pred = GRU_model.predict(scale_data[-(output_size*2):].values.reshape(1, (output_size*2), 13))\n",
    "pred = np.round(scaler.inverse_transform(pred))\n",
    "sub.평균기온 = pred[0]\n",
    "sub.to_csv(\"data/GRU_128_128_128.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
